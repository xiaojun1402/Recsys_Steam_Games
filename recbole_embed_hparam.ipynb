{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d177b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32a47a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code ref: https://www.kaggle.com/code/honglyu/bert4rec-bert-embedding-as-item-embedding\n",
    "from logging import getLogger\n",
    "from recbole.config import Config\n",
    "from recbole.data import create_dataset, data_preparation\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.utils import init_seed, init_logger\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from recbole.model.abstract_recommender import SequentialRecommender\n",
    "from recbole.model.layers import FeedForward\n",
    "\n",
    "from recbole.utils import FeatureType\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from recbole.utils import FeatureType\n",
    "import torch.nn.functional as F\n",
    "from recbole.data.interaction import Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ece8d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head Self-attention layers, a attention score dropout layer is introduced.\n",
    "    Args:\n",
    "        input_tensor (torch.Tensor): the input of the multi-head self-attention layer\n",
    "        attention_mask (torch.Tensor): the attention mask for input tensor\n",
    "    Returns:\n",
    "        hidden_states (torch.Tensor): the output of the multi-head self-attention layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads, hidden_size, hidden_dropout_prob, attn_dropout_prob, layer_norm_eps):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        if hidden_size % n_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (hidden_size, n_heads)\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = n_heads\n",
    "        self.attention_head_size = int(hidden_size / n_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        self.sqrt_attention_head_size = math.sqrt(self.attention_head_size)\n",
    "\n",
    "        self.query = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(hidden_size, self.all_head_size)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.attn_dropout = nn.Dropout(attn_dropout_prob)\n",
    "\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n",
    "        self.out_dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask, need_attention=False):\n",
    "        mixed_query_layer = self.query(input_tensor)\n",
    "        mixed_key_layer = self.key(input_tensor)\n",
    "        mixed_value_layer = self.value(input_tensor)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer).permute(0, 2, 1, 3)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer).permute(0, 2, 3, 1)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer)\n",
    "\n",
    "        attention_scores = attention_scores / self.sqrt_attention_head_size\n",
    "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "        # [batch_size heads seq_len seq_len] scores\n",
    "        # [batch_size 1 1 seq_len]\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        hidden_states = self.dense(context_layer)\n",
    "        hidden_states = self.out_dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        if need_attention:\n",
    "            return hidden_states, attention_probs\n",
    "        else:\n",
    "            return hidden_states\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    One transformer layer consists of a multi-head self-attention layer and a point-wise feed-forward layer.\n",
    "    Args:\n",
    "        hidden_states (torch.Tensor): the input of the multi-head self-attention sublayer\n",
    "        attention_mask (torch.Tensor): the attention mask for the multi-head self-attention sublayer\n",
    "    Returns:\n",
    "        feedforward_output (torch.Tensor): The output of the point-wise feed-forward sublayer,\n",
    "                                           is the output of the transformer layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, n_heads, hidden_size, intermediate_size, hidden_dropout_prob, attn_dropout_prob, hidden_act,\n",
    "        layer_norm_eps\n",
    "    ):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(\n",
    "            n_heads, hidden_size, hidden_dropout_prob, attn_dropout_prob, layer_norm_eps\n",
    "        )\n",
    "        self.feed_forward = FeedForward(hidden_size, intermediate_size, hidden_dropout_prob, hidden_act, layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, need_attention=False):\n",
    "        if need_attention:\n",
    "            attention_output, attention_probs = self.multi_head_attention(hidden_states, attention_mask, need_attention)\n",
    "        else:\n",
    "            attention_output = self.multi_head_attention(hidden_states, attention_mask, need_attention)\n",
    "        feedforward_output = self.feed_forward(attention_output)\n",
    "        if need_attention:\n",
    "            return feedforward_output, attention_probs\n",
    "        else:\n",
    "            return feedforward_output\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    r\"\"\" One TransformerEncoder consists of several TransformerLayers.\n",
    "    Args:\n",
    "        n_layers(num): num of transformer layers in transformer encoder. Default: 2\n",
    "        n_heads(num): num of attention heads for multi-head attention layer. Default: 2\n",
    "        hidden_size(num): the input and output hidden size. Default: 64\n",
    "        inner_size(num): the dimensionality in feed-forward layer. Default: 256\n",
    "        hidden_dropout_prob(float): probability of an element to be zeroed. Default: 0.5\n",
    "        attn_dropout_prob(float): probability of an attention score to be zeroed. Default: 0.5\n",
    "        hidden_act(str): activation function in feed-forward layer. Default: 'gelu'\n",
    "                      candidates: 'gelu', 'relu', 'swish', 'tanh', 'sigmoid'\n",
    "        layer_norm_eps(float): a value added to the denominator for numerical stability. Default: 1e-12\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_layers=2,\n",
    "        n_heads=2,\n",
    "        hidden_size=64,\n",
    "        inner_size=256,\n",
    "        hidden_dropout_prob=0.5,\n",
    "        attn_dropout_prob=0.5,\n",
    "        hidden_act='gelu',\n",
    "        layer_norm_eps=1e-12\n",
    "    ):\n",
    "\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        layer = TransformerLayer(\n",
    "            n_heads, hidden_size, inner_size, hidden_dropout_prob, attn_dropout_prob, hidden_act, layer_norm_eps\n",
    "        )\n",
    "        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True, need_attention=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (torch.Tensor): the input of the TransformerEncoder\n",
    "            attention_mask (torch.Tensor): the attention mask for the input hidden_states\n",
    "            output_all_encoded_layers (Bool): whether output all transformer layers' output\n",
    "        Returns:\n",
    "            all_encoder_layers (list): if output_all_encoded_layers is True, return a list consists of all transformer\n",
    "            layers' output, otherwise return a list only consists of the output of last transformer layer.\n",
    "        \"\"\"\n",
    "        all_encoder_layers = []\n",
    "        all_attention_probs = []\n",
    "        for layer_module in self.layer:\n",
    "            if need_attention:\n",
    "                hidden_states, attention_probs = layer_module(hidden_states, attention_mask, need_attention)\n",
    "            else:\n",
    "                hidden_states = layer_module(hidden_states, attention_mask)\n",
    "            if output_all_encoded_layers:\n",
    "                all_encoder_layers.append(hidden_states)\n",
    "                if need_attention:\n",
    "                    all_attention_probs.append(attention_probs)\n",
    "        if not output_all_encoded_layers:\n",
    "            all_encoder_layers.append(hidden_states)\n",
    "            if need_attention:\n",
    "                    all_attention_probs.append(attention_probs)\n",
    "        if need_attention:\n",
    "            return all_encoder_layers, all_attention_probs\n",
    "        else:\n",
    "            return all_encoder_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a36a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextSeqEmbAbstractLayer(nn.Module):\n",
    "    \"\"\"For Deep Interest Network and feature-rich sequential recommender systems, return features embedding matrices.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ContextSeqEmbAbstractLayer, self).__init__()\n",
    "        self.token_field_offsets = {}\n",
    "        self.token_embedding_table = nn.ModuleDict()\n",
    "        self.float_embedding_table = nn.ModuleDict()\n",
    "        self.token_seq_embedding_table = nn.ModuleDict()\n",
    "\n",
    "        self.token_field_names = None\n",
    "        self.token_field_dims = None\n",
    "        self.float_field_names = None\n",
    "        self.float_field_dims = None\n",
    "        self.token_seq_field_names = None\n",
    "        self.token_seq_field_dims = None\n",
    "        self.num_feature_field = None\n",
    "\n",
    "    def get_fields_name_dim(self):\n",
    "        \"\"\"get user feature field and item feature field.\n",
    "        \"\"\"\n",
    "        self.token_field_names = {type: [] for type in self.types}\n",
    "        self.token_field_dims = {type: [] for type in self.types}\n",
    "        self.float_field_names = {type: [] for type in self.types}\n",
    "        self.float_field_dims = {type: [] for type in self.types}\n",
    "        self.token_seq_field_names = {type: [] for type in self.types}\n",
    "        self.token_seq_field_dims = {type: [] for type in self.types}\n",
    "        self.num_feature_field = {type: 0 for type in self.types}\n",
    "        \n",
    "\n",
    "        for type in self.types:\n",
    "            for field_name in self.field_names[type]:\n",
    "                if self.dataset.field2type[field_name] == FeatureType.TOKEN:\n",
    "                    self.token_field_names[type].append(field_name)\n",
    "                    self.token_field_dims[type].append(self.dataset.num(field_name))\n",
    "                elif self.dataset.field2type[field_name] == FeatureType.TOKEN_SEQ:\n",
    "                    self.token_seq_field_names[type].append(field_name)\n",
    "                    self.token_seq_field_dims[type].append(self.dataset.num(field_name))\n",
    "                else:\n",
    "                    self.float_field_names[type].append(field_name)\n",
    "                    self.float_field_dims[type].append(self.dataset.num(field_name))\n",
    "                self.num_feature_field[type] += 1\n",
    "                \n",
    "    def get_embedding(self):\n",
    "        \"\"\"get embedding of all features.\n",
    "        \"\"\"\n",
    "        for type in self.types:\n",
    "            if len(self.token_field_dims[type]) > 0:\n",
    "                self.token_field_offsets[type] = np.array((0, *np.cumsum(self.token_field_dims[type])[:-1]),\n",
    "                                                          dtype=np.long)\n",
    "                self.token_embedding_table[type] = FMEmbedding(\n",
    "                    self.token_field_dims[type], self.token_field_offsets[type], self.embedding_size\n",
    "                ).to(self.device)\n",
    "            if len(self.float_field_dims[type]) > 0:\n",
    "                self.float_embedding_table[type] = nn.Embedding(\n",
    "                    np.sum(self.float_field_dims[type], dtype=np.int32), self.embedding_size\n",
    "                ).to(self.device)\n",
    "            if len(self.token_seq_field_dims) > 0:\n",
    "                self.token_seq_embedding_table[type] = nn.ModuleList()\n",
    "                for token_seq_field_dim in self.token_seq_field_dims[type]:\n",
    "                    self.token_seq_embedding_table[type].append(\n",
    "                        nn.Embedding(token_seq_field_dim, self.embedding_size).to(self.device)\n",
    "                    )\n",
    "\n",
    "    def embed_float_fields(self, float_fields, type, embed=True):\n",
    "        \"\"\"Get the embedding of float fields.\n",
    "        In the following three functions(\"embed_float_fields\" \"embed_token_fields\" \"embed_token_seq_fields\")\n",
    "        when the type is user, [batch_size, max_item_length] should be recognised as [batch_size]\n",
    "        Args:\n",
    "            float_fields(torch.Tensor): [batch_size, max_item_length, num_float_field]\n",
    "            type(str): user or item\n",
    "            embed(bool): embed or not\n",
    "        Returns:\n",
    "            torch.Tensor: float fields embedding. [batch_size, max_item_length, num_float_field, embed_dim]\n",
    "        \"\"\"\n",
    "        if not embed or float_fields is None:\n",
    "            return float_fields\n",
    "\n",
    "        num_float_field = float_fields.shape[-1]\n",
    "\n",
    "        index = torch.arange(0, num_float_field).unsqueeze(0).expand_as(float_fields).long().to(self.device)\n",
    "        float_embedding = self.float_embedding_table[type](index)\n",
    "        float_embedding = torch.mul(float_embedding, float_fields.unsqueeze(-1))\n",
    "\n",
    "        return float_embedding\n",
    "\n",
    "    def embed_token_fields(self, token_fields, type):\n",
    "        \"\"\"Get the embedding of token fields\n",
    "        Args:\n",
    "            token_fields(torch.Tensor): input, [batch_size, max_item_length, num_token_field]\n",
    "            type(str): user or item\n",
    "        Returns:\n",
    "            torch.Tensor: token fields embedding, [batch_size, max_item_length, num_token_field, embed_dim]\n",
    "        \"\"\"\n",
    "        if token_fields is None:\n",
    "            return None\n",
    "        # [batch_size, max_item_length, num_token_field, embed_dim]\n",
    "        if type == 'item':\n",
    "            embedding_shape = token_fields.shape + (-1,)\n",
    "            token_fields = token_fields.reshape(-1, token_fields.shape[-1])\n",
    "            token_embedding = self.token_embedding_table[type](token_fields)\n",
    "            token_embedding = token_embedding.view(embedding_shape)\n",
    "        else:\n",
    "            token_embedding = self.token_embedding_table[type](token_fields)\n",
    "        return token_embedding\n",
    "\n",
    "    def embed_token_seq_fields(self, token_seq_fields, type):\n",
    "        \"\"\"Get the embedding of token_seq fields.\n",
    "        Args:\n",
    "            token_seq_fields(torch.Tensor): input, [batch_size, max_item_length, seq_len]`\n",
    "            type(str): user or item\n",
    "            mode(str): mean/max/sum\n",
    "        Returns:\n",
    "            torch.Tensor: result [batch_size, max_item_length, num_token_seq_field, embed_dim]\n",
    "        \"\"\"\n",
    "        fields_result = []\n",
    "        for i, token_seq_field in enumerate(token_seq_fields):\n",
    "            embedding_table = self.token_seq_embedding_table[type][i]\n",
    "            mask = token_seq_field != 0  # [batch_size, max_item_length, seq_len]\n",
    "            mask = mask.float()\n",
    "            value_cnt = torch.sum(mask, dim=-1, keepdim=True)  # [batch_size, max_item_length, 1]\n",
    "            token_seq_embedding = embedding_table(token_seq_field)  # [batch_size, max_item_length, seq_len, embed_dim]\n",
    "            mask = mask.unsqueeze(-1).expand_as(token_seq_embedding)\n",
    "            if self.pooling_mode == 'max':\n",
    "                masked_token_seq_embedding = token_seq_embedding - (1 - mask) * 1e9\n",
    "                result = torch.max(\n",
    "                    masked_token_seq_embedding, dim=-2, keepdim=True\n",
    "                )  # [batch_size, max_item_length, 1, embed_dim]\n",
    "                result = result.values\n",
    "            elif self.pooling_mode == 'sum':\n",
    "                masked_token_seq_embedding = token_seq_embedding * mask.float()\n",
    "                result = torch.sum(\n",
    "                    masked_token_seq_embedding, dim=-2, keepdim=True\n",
    "                )  # [batch_size, max_item_length, 1, embed_dim]\n",
    "            else:\n",
    "                masked_token_seq_embedding = token_seq_embedding * mask.float()\n",
    "                result = torch.sum(masked_token_seq_embedding, dim=-2)  # [batch_size, max_item_length, embed_dim]\n",
    "                eps = torch.FloatTensor([1e-8]).to(self.device)\n",
    "                result = torch.div(result, value_cnt + eps)  # [batch_size, max_item_length, embed_dim]\n",
    "                result = result.unsqueeze(-2)  # [batch_size, max_item_length, 1, embed_dim]\n",
    "\n",
    "            fields_result.append(result)\n",
    "        if len(fields_result) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return torch.cat(fields_result, dim=-2)  # [batch_size, max_item_length, num_token_seq_field, embed_dim]\n",
    "\n",
    "    def embed_input_fields(self, user_idx, item_idx):\n",
    "        \"\"\"Get the embedding of user_idx and item_idx\n",
    "        Args:\n",
    "            user_idx(torch.Tensor): interaction['user_id']\n",
    "            item_idx(torch.Tensor): interaction['item_id_list']\n",
    "        Returns:\n",
    "            dict: embedding of user feature and item feature\n",
    "        \"\"\"\n",
    "        user_item_feat = {'user': self.user_feat, 'item': self.item_feat}\n",
    "        user_item_idx = {'user': user_idx, 'item': item_idx}\n",
    "        float_fields_embedding = {}\n",
    "        token_fields_embedding = {}\n",
    "        token_seq_fields_embedding = {}\n",
    "        sparse_embedding = {}\n",
    "        dense_embedding = {}\n",
    "\n",
    "        for type in self.types:\n",
    "            float_fields = []\n",
    "            for field_name in self.float_field_names[type]:\n",
    "                feature = user_item_feat[type][field_name][user_item_idx[type]]\n",
    "                float_fields.append(feature if len(feature.shape) == (2 + (type == 'item')) else feature.unsqueeze(-1))\n",
    "            if len(float_fields) > 0:\n",
    "                float_fields = torch.cat(float_fields, dim=-1)  # [batch_size, max_item_length, num_float_field]\n",
    "            else:\n",
    "                float_fields = None\n",
    "            float_fields_embedding[type] = self.embed_float_fields(float_fields, type)\n",
    "\n",
    "            token_fields = []\n",
    "            for field_name in self.token_field_names[type]:\n",
    "                feature = user_item_feat[type][field_name][user_item_idx[type]]\n",
    "                token_fields.append(feature.unsqueeze(-1))\n",
    "            if len(token_fields) > 0:\n",
    "                token_fields = torch.cat(token_fields, dim=-1)  # [batch_size, max_item_length, num_token_field]\n",
    "            else:\n",
    "                token_fields = None\n",
    "            token_fields_embedding[type] = self.embed_token_fields(token_fields, type)\n",
    "\n",
    "            token_seq_fields = []\n",
    "            for field_name in self.token_seq_field_names[type]:\n",
    "                feature = user_item_feat[type][field_name][user_item_idx[type]]\n",
    "                token_seq_fields.append(feature)\n",
    "            token_seq_fields_embedding[type] = self.embed_token_seq_fields(token_seq_fields, type)\n",
    "\n",
    "            if token_fields_embedding[type] is None:\n",
    "                sparse_embedding[type] = token_seq_fields_embedding[type]\n",
    "            else:\n",
    "                if token_seq_fields_embedding[type] is None:\n",
    "                    sparse_embedding[type] = token_fields_embedding[type]\n",
    "                else:\n",
    "                    sparse_embedding[type] = torch.cat([token_fields_embedding[type], token_seq_fields_embedding[type]],\n",
    "                                                       dim=-2)\n",
    "            dense_embedding[type] = float_fields_embedding[type]\n",
    "\n",
    "        return sparse_embedding, dense_embedding\n",
    "\n",
    "    def forward(self, user_idx, item_idx):\n",
    "        return self.embed_input_fields(user_idx, item_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "809571e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSeqEmbLayer(ContextSeqEmbAbstractLayer):\n",
    "    \"\"\"For feature-rich sequential recommenders, return item features embedding matrices according to\n",
    "    selected features.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, embedding_size, selected_features, pooling_mode, device, mask=False):\n",
    "        super(FeatureSeqEmbLayer, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.embedding_size = embedding_size\n",
    "        self.dataset = dataset\n",
    "        self.user_feat = None\n",
    "        print('device', self.device)\n",
    "        self.item_feat = self.dataset.get_item_feature().to(self.device)\n",
    "        \n",
    "        if mask:\n",
    "            feat =  self.item_feat\n",
    "            new_feat = {}\n",
    "            for key in feat.interaction.keys():\n",
    "                item_ = feat.interaction.get(key)\n",
    "                if key == 'item_id':\n",
    "                    new_feat[key] = torch.cat((item_, torch.tensor(item_.shape).to(device)))\n",
    "                else:\n",
    "                    new_feat[key] = F.pad(item_, (0, 0, 0, 1))\n",
    "            self.item_feat = Interaction(new_feat)\n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "        self.field_names = {'item': selected_features}\n",
    "\n",
    "        self.types = ['item']\n",
    "        self.pooling_mode = pooling_mode\n",
    "        try:\n",
    "            assert self.pooling_mode in ['mean', 'max', 'sum']\n",
    "        except AssertionError:\n",
    "            raise AssertionError(\"Make sure 'pooling_mode' in ['mean', 'max', 'sum']!\")\n",
    "        self.get_fields_name_dim()\n",
    "        self.get_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b13f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT4RecF(SequentialRecommender):\n",
    "\n",
    "    def __init__(self, config, dataset):\n",
    "        super(BERT4RecF, self).__init__(config, dataset)\n",
    "\n",
    "        # load parameters info\n",
    "        self.n_layers = config['n_layers']\n",
    "        self.n_heads = config['n_heads']\n",
    "        self.hidden_size = config['hidden_size']  # same as embedding_size\n",
    "        self.inner_size = config['inner_size']  # the dimensionality in feed-forward layer\n",
    "        self.hidden_dropout_prob = config['hidden_dropout_prob']\n",
    "        self.attn_dropout_prob = config['attn_dropout_prob']\n",
    "        self.hidden_act = config['hidden_act']\n",
    "        self.layer_norm_eps = config['layer_norm_eps']\n",
    "\n",
    "        self.mask_ratio = config['mask_ratio']\n",
    "\n",
    "        self.loss_type = config['loss_type']\n",
    "        self.initializer_range = config['initializer_range']\n",
    "\n",
    "        # add feature selection parameters\n",
    "        \n",
    "        # load dataset info\n",
    "        self.mask_token = self.n_items\n",
    "        self.mask_item_length = int(self.mask_ratio * self.max_seq_length)\n",
    "\n",
    "        # define layers and loss\n",
    "        self.selected_features = config['selected_features']\n",
    "        self.pooling_mode = config['pooling_mode']\n",
    "        self.device = config['device']\n",
    "        self.num_feature_field = sum(\n",
    "            1 if dataset.field2type[field] != FeatureType.FLOAT_SEQ else dataset.num(field)\n",
    "            for field in config['selected_features']\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.item_embedding = nn.Embedding(self.n_items + 1, self.hidden_size, padding_idx=0)\n",
    "        self.position_embedding = nn.Embedding(self.max_seq_length + 1, self.hidden_size)  # add mask_token at the last\n",
    "        self.feature_embed_layer = FeatureSeqEmbLayer(\n",
    "            dataset, self.hidden_size, self.selected_features, self.pooling_mode, self.device, mask=True\n",
    "        )\n",
    "    \n",
    "        self.trm_encoder = TransformerEncoder(\n",
    "            n_layers=self.n_layers,\n",
    "            n_heads=self.n_heads,\n",
    "            hidden_size=self.hidden_size,\n",
    "            inner_size=self.inner_size,\n",
    "            hidden_dropout_prob=self.hidden_dropout_prob,\n",
    "            attn_dropout_prob=self.attn_dropout_prob,\n",
    "            hidden_act=self.hidden_act,\n",
    "            layer_norm_eps=self.layer_norm_eps\n",
    "        )\n",
    "        \n",
    "        self.concat_layer = nn.Linear(self.hidden_size * (1 + self.num_feature_field), self.hidden_size)\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(self.hidden_dropout_prob)\n",
    "\n",
    "        # we only need compute the loss at the masked position\n",
    "        try:\n",
    "            assert self.loss_type in ['BPR', 'CE']\n",
    "        except AssertionError:\n",
    "            raise AssertionError(\"Make sure 'loss_type' in ['BPR', 'CE']!\")\n",
    "\n",
    "        # parameters initialization\n",
    "        self.apply(self._init_weights)\n",
    "        self.other_parameter_name = ['feature_embed_layer']\n",
    "#         print('num_feature_field', self.num_feature_field)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\" Initialize the weights \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def _neg_sample(self, item_set):\n",
    "        item = random.randint(1, self.n_items - 1)\n",
    "        while item in item_set:\n",
    "            item = random.randint(1, self.n_items - 1)\n",
    "        return item\n",
    "\n",
    "    def _padding_sequence(self, sequence, max_length):\n",
    "        pad_len = max_length - len(sequence)\n",
    "        sequence = [0] * pad_len + sequence\n",
    "        sequence = sequence[-max_length:]  # truncate according to the max_length\n",
    "        return sequence\n",
    "\n",
    "    def reconstruct_train_data(self, item_seq):\n",
    "        \"\"\"\n",
    "        Mask item sequence for training.\n",
    "        \"\"\"\n",
    "        device = item_seq.device\n",
    "        batch_size = item_seq.size(0)\n",
    "\n",
    "        sequence_instances = item_seq.cpu().numpy().tolist()\n",
    "\n",
    "        # Masked Item Prediction\n",
    "        # [B * Len]\n",
    "        masked_item_sequence = []\n",
    "        pos_items = []\n",
    "        neg_items = []\n",
    "        masked_index = []\n",
    "#         print('self.mask_token', self.mask_token)\n",
    "        for instance in sequence_instances:\n",
    "            # WE MUST USE 'copy()' HERE!\n",
    "            masked_sequence = instance.copy()\n",
    "            pos_item = []\n",
    "            neg_item = []\n",
    "            index_ids = []\n",
    "            for index_id, item in enumerate(instance):\n",
    "                # padding is 0, the sequence is end\n",
    "                if item == 0:\n",
    "                    break\n",
    "                prob = random.random()\n",
    "                if prob < self.mask_ratio:\n",
    "                    pos_item.append(item)\n",
    "                    neg_item.append(self._neg_sample(instance))\n",
    "                    masked_sequence[index_id] = self.mask_token\n",
    "                    index_ids.append(index_id)\n",
    "            \n",
    "\n",
    "            masked_item_sequence.append(masked_sequence)\n",
    "            pos_items.append(self._padding_sequence(pos_item, self.mask_item_length))\n",
    "            neg_items.append(self._padding_sequence(neg_item, self.mask_item_length))\n",
    "            masked_index.append(self._padding_sequence(index_ids, self.mask_item_length))\n",
    "\n",
    "        # [B Len]\n",
    "        masked_item_sequence = torch.tensor(masked_item_sequence, dtype=torch.long, device=device).view(batch_size, -1)\n",
    "        # [B mask_len]\n",
    "        pos_items = torch.tensor(pos_items, dtype=torch.long, device=device).view(batch_size, -1)\n",
    "        # [B mask_len]\n",
    "        neg_items = torch.tensor(neg_items, dtype=torch.long, device=device).view(batch_size, -1)\n",
    "        # [B mask_len]\n",
    "        masked_index = torch.tensor(masked_index, dtype=torch.long, device=device).view(batch_size, -1)\n",
    "        return masked_item_sequence, pos_items, neg_items, masked_index\n",
    "\n",
    "    def reconstruct_test_data(self, item_seq, item_seq_len):\n",
    "        \"\"\"\n",
    "        Add mask token at the last position according to the lengths of item_seq\n",
    "        \"\"\"\n",
    "        padding = torch.zeros(item_seq.size(0), dtype=torch.long, device=item_seq.device)  # [B]\n",
    "        item_seq = torch.cat((item_seq, padding.unsqueeze(-1)), dim=-1)  # [B max_len+1]\n",
    "        for batch_id, last_position in enumerate(item_seq_len):\n",
    "            item_seq[batch_id][last_position] = self.mask_token\n",
    "        return item_seq\n",
    "\n",
    "    def forward(self, item_seq, need_attention=False):\n",
    "        position_ids = torch.arange(item_seq.size(1), dtype=torch.long, device=item_seq.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(item_seq)\n",
    "        position_embedding = self.position_embedding(position_ids)\n",
    "        \n",
    "    \n",
    "        item_emb = self.item_embedding(item_seq)\n",
    "#         print('item_emb size', item_emb.shape)\n",
    "#         print('item_seq', item_seq.shape)\n",
    "        \n",
    "        \n",
    "        sparse_embedding, dense_embedding = self.feature_embed_layer(None, item_seq)\n",
    "        sparse_embedding = sparse_embedding['item']\n",
    "        dense_embedding = dense_embedding['item']\n",
    "        # concat the sparse embedding and float embedding\n",
    "        feature_table = []\n",
    "        if sparse_embedding is not None:\n",
    "            feature_table.append(sparse_embedding)\n",
    "        if dense_embedding is not None:\n",
    "            feature_table.append(dense_embedding)\n",
    "\n",
    "        feature_table = torch.cat(feature_table, dim=-2)\n",
    "        table_shape = feature_table.shape\n",
    "        feat_num, embedding_size = table_shape[-2], table_shape[-1]\n",
    "        feature_emb = feature_table.view(table_shape[:-2] + (feat_num * embedding_size,))\n",
    "        input_concat = torch.cat((item_emb, feature_emb), -1)  # [B 1+field_num*H]\n",
    "\n",
    "        input_emb = self.concat_layer(input_concat)\n",
    "        input_emb = input_emb + position_embedding\n",
    "        input_emb = self.LayerNorm(input_emb)\n",
    "        input_emb = self.dropout(input_emb)\n",
    "        extended_attention_mask = self.get_attention_mask(item_seq, bidirectional=True)\n",
    "        if need_attention:\n",
    "            trm_output, attention_weights = self.trm_encoder(input_emb, extended_attention_mask, output_all_encoded_layers=True, need_attention=True)\n",
    "        else:\n",
    "            trm_output = self.trm_encoder(input_emb, extended_attention_mask, output_all_encoded_layers=True)\n",
    "        output = trm_output[-1]\n",
    "        if need_attention:\n",
    "            return output, attention_weights\n",
    "        else:\n",
    "            return output  # [B L H]\n",
    "\n",
    "    def multi_hot_embed(self, masked_index, max_length):\n",
    "        \"\"\"\n",
    "        For memory, we only need calculate loss for masked position.\n",
    "        Generate a multi-hot vector to indicate the masked position for masked sequence, and then is used for\n",
    "        gathering the masked position hidden representation.\n",
    "        Examples:\n",
    "            sequence: [1 2 3 4 5]\n",
    "            masked_sequence: [1 mask 3 mask 5]\n",
    "            masked_index: [1, 3]\n",
    "            max_length: 5\n",
    "            multi_hot_embed: [[0 1 0 0 0], [0 0 0 1 0]]\n",
    "        \"\"\"\n",
    "        masked_index = masked_index.view(-1)\n",
    "        multi_hot = torch.zeros(masked_index.size(0), max_length, device=masked_index.device)\n",
    "        multi_hot[torch.arange(masked_index.size(0)), masked_index] = 1\n",
    "        return multi_hot\n",
    "\n",
    "    def calculate_loss(self, interaction):\n",
    "        item_seq = interaction[self.ITEM_SEQ]\n",
    "        masked_item_seq, pos_items, neg_items, masked_index = self.reconstruct_train_data(item_seq)\n",
    "\n",
    "        seq_output = self.forward(masked_item_seq, need_attention=False)\n",
    "        pred_index_map = self.multi_hot_embed(masked_index, masked_item_seq.size(-1))  # [B*mask_len max_len]\n",
    "        # [B mask_len] -> [B mask_len max_len] multi hot\n",
    "        pred_index_map = pred_index_map.view(masked_index.size(0), masked_index.size(1), -1)  # [B mask_len max_len]\n",
    "        # [B mask_len max_len] * [B max_len H] -> [B mask_len H]\n",
    "        # only calculate loss for masked position\n",
    "        seq_output = torch.bmm(pred_index_map, seq_output)  # [B mask_len H]\n",
    "\n",
    "        if self.loss_type == 'BPR':\n",
    "            pos_items_emb = self.item_embedding(pos_items)  # [B mask_len H]\n",
    "            neg_items_emb = self.item_embedding(neg_items)  # [B mask_len H]\n",
    "            pos_score = torch.sum(seq_output * pos_items_emb, dim=-1)  # [B mask_len]\n",
    "            neg_score = torch.sum(seq_output * neg_items_emb, dim=-1)  # [B mask_len]\n",
    "            targets = (masked_index > 0).float()\n",
    "            loss = - torch.sum(torch.log(1e-14 + torch.sigmoid(pos_score - neg_score)) * targets) \\\n",
    "                   / torch.sum(targets)\n",
    "            return loss\n",
    "\n",
    "        elif self.loss_type == 'CE':\n",
    "            loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "#             test_item_emb = self.item_embedding_transformation(self.item_embedding.weight)[:self.n_items]  # [item_num H]\n",
    "            test_item_emb = self.item_embedding.weight[:self.n_items]\n",
    "            logits = torch.matmul(seq_output, test_item_emb.transpose(0, 1))  # [B mask_len item_num]\n",
    "            targets = (masked_index > 0).float().view(-1)  # [B*mask_len]\n",
    "\n",
    "            loss = torch.sum(loss_fct(logits.view(-1, test_item_emb.size(0)), pos_items.view(-1)) * targets) \\\n",
    "                   / torch.sum(targets)\n",
    "            return loss\n",
    "        else:\n",
    "            raise NotImplementedError(\"Make sure 'loss_type' in ['BPR', 'CE']!\")\n",
    "\n",
    "    def predict(self, interaction):\n",
    "        item_seq = interaction[self.ITEM_SEQ]\n",
    "        item_seq_len = interaction[self.ITEM_SEQ_LEN]\n",
    "        test_item = interaction[self.ITEM_ID]\n",
    "        item_seq = self.reconstruct_test_data(item_seq, item_seq_len)\n",
    "        seq_output = self.forward(item_seq)\n",
    "        seq_output = self.gather_indexes(seq_output, item_seq_len)  # [B H]\n",
    "        test_item_emb = self.item_embedding(test_item)\n",
    "        scores = torch.mul(seq_output, test_item_emb).sum(dim=1)  # [B]\n",
    "        return scores\n",
    "\n",
    "    def full_sort_predict(self, interaction):\n",
    "        item_seq = interaction[self.ITEM_SEQ]\n",
    "        item_seq_len = interaction[self.ITEM_SEQ_LEN]\n",
    "        item_seq = self.reconstruct_test_data(item_seq, item_seq_len)\n",
    "        seq_output, attention_weights = self.forward(item_seq, need_attention=True)\n",
    "        print(attention_weights[0].shape)\n",
    "        seq_output = self.gather_indexes(seq_output, item_seq_len)  # [B H]\n",
    "        test_items_emb = self.item_embedding.weight[:self.n_items]  # delete masked token\n",
    "        scores = torch.matmul(seq_output, test_items_emb.transpose(0, 1))  # [B, item_num]\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5878e435",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    \"data_path\": \"data\",\n",
    "    \"dataset\": \"steam_data\",\n",
    "    \"model\": \"BERT4Rec\",\n",
    "    \"show_progress\": True,\n",
    "\n",
    "    \"USER_ID_FIELD\": \"user_id\",\n",
    "    \"ITEM_ID_FIELD\": \"item_id\",\n",
    "    \"RATING_FIELD\": \"is_recommended\",\n",
    "    \"TIME_FIELD\": \"timestamp\",\n",
    "\n",
    "    \"load_col\": {\n",
    "        \"inter\": [\"user_id\", \"item_id\", \"is_recommended\", \"timestamp\"],\n",
    "        \"item\": [\"item_id\", \"item_emb\"],\n",
    "    },\n",
    "    'selected_features': ['item_emb'],\n",
    "    \"metrics\": [\"Recall\", \"MRR\", \"NDCG\"],\n",
    "    'neg_sampling': None,\n",
    "    'train_neg_sample_args': None,\n",
    "    'epochs': 1,\n",
    "    'train_batch_size': 256,\n",
    "    'n_layers': 2,\n",
    "    'n_heads': 2,\n",
    "    'hidden_size': 64,\n",
    "    'inner_size': 256,\n",
    "    'hidden_dropout_prob': 0.5,\n",
    "    'attn_dropout_prob': 0.5,\n",
    "    'hidden_act': 'gelu',\n",
    "    'layer_norm_eps': 1e-12,\n",
    "    'initializer_range': 0.02,\n",
    "    'mask_ratio': 0.2,\n",
    "    'loss_type': 'CE',\n",
    "    'learning_rate': 0.002,\n",
    "    'pooling_mode': 'sum',\n",
    "    'eval_batch_size': 256,\n",
    "    'eval_args': {\n",
    "        'split': {'RS': [0.8, 0.2, 0]},\n",
    "        'group_by': 'user',\n",
    "        'order': 'TO',\n",
    "        'mode': 'full'},\n",
    "    'gpu_id': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a19ce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(model=BERT4RecF, dataset='steam_data', config_dict=config_dict)\n",
    "init_seed(config['seed'], config['reproducibility'])\n",
    "dataset = create_dataset(config)\n",
    "train_data, valid_data, test_data = data_preparation(config, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81c2fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model loading and initialization\n",
    "model = BERT4RecF(config, train_data.dataset).to(config['device'])\n",
    "\n",
    "# trainer loading and initialization\n",
    "trainer = Trainer(config, model)\n",
    "\n",
    "# model training\n",
    "best_valid_score, best_valid_result = trainer.fit(train_data=train_data,\n",
    "                                                  show_progress=True,\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a160bcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(eval_data=valid_data, load_best_model=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ede7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from recbole.quick_start import load_data_and_model\n",
    "from recbole.data.interaction import Interaction\n",
    "\n",
    "checkpoint_path = r'/home/test_scripts/rs/saved/best.pth'\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "config = checkpoint['config']\n",
    "init_seed(config['seed'], config['reproducibility'])\n",
    "init_logger(config)\n",
    "logger = getLogger()\n",
    "logger.info(config)\n",
    "model = BERT4RecF(config, train_data.dataset).to(config['device'])\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.load_other_parameter(checkpoint.get('other_parameter'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0017cd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recbole.utils.case_study import full_sort_topk\n",
    "from recbole.quick_start.quick_start import load_data_and_model\n",
    "# config, model, dataset, train_data, valid_data, test_data = load_data_and_model(\n",
    "#     model_file=checkpoint_path,\n",
    "# )\n",
    "external_user_ids = dataset.id2token(\n",
    "    dataset.uid_field, list(range(dataset.user_num)))[1:]#fist element in array is 'PAD'(default of Recbole) ->remove it\n",
    "\n",
    "import torch\n",
    "from recbole.data.interaction import Interaction\n",
    "\n",
    "def add_last_item(old_interaction, last_item_id, max_len=50):\n",
    "    new_seq_items = old_interaction['item_id_list'][-1]\n",
    "    if old_interaction['item_length'][-1].item() < max_len:\n",
    "        new_seq_items[old_interaction['item_length'][-1].item()] = last_item_id\n",
    "    else:\n",
    "        new_seq_items = torch.roll(new_seq_items, -1)\n",
    "        new_seq_items[-1] = last_item_id\n",
    "    return new_seq_items.view(1, len(new_seq_items))\n",
    "\n",
    "def predict_for_all_item(external_user_id, dataset, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        uid_series = dataset.token2id(dataset.uid_field, [external_user_id])\n",
    "        index = np.isin(dataset.inter_feat[dataset.uid_field].numpy(), uid_series)\n",
    "        input_interaction = dataset[index]\n",
    "        test = {\n",
    "            'item_id_list': add_last_item(input_interaction, \n",
    "                                          input_interaction['item_id'][-1].item(), model.max_seq_length),\n",
    "            'item_length': torch.tensor(\n",
    "                [input_interaction['item_length'][-1].item() + 1\n",
    "                 if input_interaction['item_length'][-1].item() < model.max_seq_length else model.max_seq_length])\n",
    "        }\n",
    "        new_inter = Interaction(test)\n",
    "        new_inter = new_inter.to(config['device'])\n",
    "        new_scores, attention = model.full_sort_predict(new_inter)\n",
    "        new_scores = new_scores.view(-1, test_data.dataset.item_num)\n",
    "        new_scores[:, 0] = -np.inf  # set scores of [pad] to -inf\n",
    "    return torch.topk(new_scores, 12)[1], attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "521af014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing user 5557878: token [5557878] is not existed in user_id\n",
      "Skipping user 5227423 — no interaction data.\n",
      "Error processing user 51580: token [51580] is not existed in user_id\n",
      "Error processing user 76583: token [76583] is not existed in user_id\n",
      "Error processing user 271318: token [271318] is not existed in user_id\n",
      "\n",
      "✅ Total users processed: 0\n",
      "⛔ Total users skipped: 5\n"
     ]
    }
   ],
   "source": [
    "pred_list = [\"5557878\", \"5227423\", \"51580\", \"76583\", \"271318\"]\n",
    "\n",
    "topk_items = []\n",
    "skipped_users = []\n",
    "\n",
    "for external_user_id in pred_list:\n",
    "    try:\n",
    "        # Convert external ID to internal ID\n",
    "        internal_uid = dataset.token2id(dataset.uid_field, [external_user_id])[0]\n",
    "\n",
    "        # Filter dataset to get user's interaction\n",
    "        index = np.isin(dataset.inter_feat[dataset.uid_field].numpy(), internal_uid)\n",
    "        input_interaction = dataset[index]\n",
    "\n",
    "        if len(input_interaction) == 0:\n",
    "            print(f\"Skipping user {external_user_id} — no interaction data.\")\n",
    "            skipped_users.append(external_user_id)\n",
    "            continue\n",
    "\n",
    "        # Extract last item and sequence length\n",
    "        last_item_id = input_interaction['item_id'][-1].item()\n",
    "        last_item_length = input_interaction['item_length'][-1].item()\n",
    "\n",
    "        # Prepare input sequence\n",
    "        def add_last_item(interaction, last_item_id, max_seq_length):\n",
    "            item_seq = interaction['item_id_list'][-1].tolist()\n",
    "            item_seq.append(last_item_id)\n",
    "            if len(item_seq) > max_seq_length:\n",
    "                item_seq = item_seq[-max_seq_length:]\n",
    "            else:\n",
    "                item_seq = [0] * (max_seq_length - len(item_seq)) + item_seq\n",
    "            return torch.tensor([item_seq], dtype=torch.long)\n",
    "\n",
    "        new_seq = add_last_item(input_interaction, last_item_id, model.max_seq_length)\n",
    "        new_len = torch.tensor([\n",
    "            last_item_length + 1 if last_item_length < model.max_seq_length else model.max_seq_length\n",
    "        ])\n",
    "\n",
    "        # Build Interaction object\n",
    "        test_input = {\n",
    "            'item_id_list': new_seq,\n",
    "            'item_length': new_len\n",
    "        }\n",
    "        new_inter = Interaction(test_input).to(config['device'])\n",
    "\n",
    "        # Predict\n",
    "        scores, _ = model.full_sort_predict(new_inter)\n",
    "\n",
    "        # Top-10 prediction\n",
    "        topk_iid = torch.topk(scores, k=10, dim=1).indices[0]\n",
    "        external_item_list = dataset.id2token(dataset.iid_field, topk_iid.cpu()).tolist()\n",
    "        topk_items.append(external_item_list)\n",
    "\n",
    "        print(f\"Top 10 items for user {external_user_id}: {external_item_list}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing user {external_user_id}: {e}\")\n",
    "        skipped_users.append(external_user_id)\n",
    "\n",
    "print(f\"\\n✅ Total users processed: {len(topk_items)}\")\n",
    "print(f\"⛔ Total users skipped: {len(skipped_users)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f00cd288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\recsys\\rec\\lib\\site-packages\\recbole\\data\\dataset\\dataset.py:501: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[field].fillna(value=\"\", inplace=True)\n",
      "f:\\recsys\\rec\\lib\\site-packages\\recbole\\data\\dataset\\dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  feat[field].fillna(value=0, inplace=True)\n",
      "f:\\recsys\\rec\\lib\\site-packages\\recbole\\data\\dataset\\dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
      "19 Jun 19:36    INFO  \u001b[1;35m[Training]: \u001b[0m\u001b[1;36mtrain_batch_size\u001b[0m = \u001b[1;33m[256]\u001b[0m\u001b[1;36m train_neg_sample_args\u001b[0m: \u001b[1;33m[{'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}]\u001b[0m\n",
      "\n",
      "19 Jun 19:36    INFO  \u001b[1;35m[Evaluation]: \u001b[0m\u001b[1;36meval_batch_size\u001b[0m = \u001b[1;33m[16]\u001b[0m\u001b[1;36m eval_args\u001b[0m: \u001b[1;33m[{'split': {'RS': [0.8, 0.2, 0]}, 'order': 'TO', 'group_by': 'user', 'mode': {'valid': 'uni100', 'test': 'uni100'}}]\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = Config(model=BERT4RecF, config_dict=config_dict)\n",
    "dataset = create_dataset(config)\n",
    "train_data, valid_data, test_data = data_preparation(config, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ccf619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HPARAM\n",
    "from recbole.config import Config\n",
    "from recbole.data import create_dataset, data_preparation\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.utils import init_seed, init_logger\n",
    "\n",
    "def objective_function(config_dict=None, config_file_list=None):\n",
    "    config = Config(model=BERT4RecF, config_dict=config_dict, config_file_list=config_file_list)\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "    init_logger(config)\n",
    "    print(config)\n",
    "\n",
    "    # Dataset and dataloader\n",
    "    dataset = create_dataset(config)\n",
    "    train_data, valid_data, test_data = data_preparation(config, dataset)\n",
    "\n",
    "    # Load your custom model\n",
    "    model = BERT4RecF(config, train_data.dataset).to(config['device'])\n",
    "\n",
    "    # Trainer (default or custom)\n",
    "    trainer = Trainer(config, model)\n",
    "\n",
    "    # Train\n",
    "    best_valid_score, best_valid_result = trainer.fit(train_data=train_data, show_progress=config['show_progress'])\n",
    "\n",
    "    # Evaluate\n",
    "    test_result = trainer.evaluate(valid_data, load_best_model=True, show_progress=config['show_progress'])\n",
    "\n",
    "    return {\n",
    "        'score': best_valid_score,\n",
    "        'result': test_result\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "239bc706",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_dict = {\n",
    "    \"loguniform\": {\n",
    "        \"learning_rate\": [-8, 0]\n",
    "    },\n",
    "    \"choice\": {\n",
    "        \"hidden_size\": [64, 128, 256],\n",
    "        \"n_layers\": [2, 3, 4],\n",
    "        \"n_heads\": [2, 4, 8]\n",
    "    },\n",
    "    \"uniform\": {\n",
    "        \"mask_ratio\": [0.15, 0.3]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "02e3c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recbole.trainer import HyperTuning\n",
    "\n",
    "hp = HyperTuning(\n",
    "    objective_function=objective_function,\n",
    "    algo='random',  # or 'exhaustive'\n",
    "    early_stop=10,\n",
    "    max_evals=50,\n",
    "    params_dict=parameter_dict,\n",
    "    fixed_config_file_list=['./data/bert4rec.yaml']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb95a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "hp.run()\n",
    "# export result to the file\n",
    "hp.export_result(output_file='hyperparam.result')\n",
    "# print best parameters\n",
    "print('best params: ', hp.best_params)\n",
    "# print best result\n",
    "print('best result: ')\n",
    "print(hp.params2result[hp.params2str(hp.best_params)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
